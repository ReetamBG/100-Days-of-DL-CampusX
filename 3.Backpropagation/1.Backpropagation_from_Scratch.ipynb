{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30822,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-01-02T11:30:36.719307Z","iopub.execute_input":"2025-01-02T11:30:36.719627Z","iopub.status.idle":"2025-01-02T11:30:37.191578Z","shell.execute_reply.started":"2025-01-02T11:30:36.719595Z","shell.execute_reply":"2025-01-02T11:30:37.190481Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"# Flow of Training a neural network\n\n1. Initialize Parameters\n2. For each epoch:\n   - For every sample:\n       - Forward Propagation (Predict Output)\n       - Calculate Loss\n       - Update weights and biases","metadata":{}},{"cell_type":"code","source":"# Let us create our data\n\ndf = pd.DataFrame([[8, 8, 4], [7, 9, 5],[6,10,6],[5,12,7]], columns=['cgpa', 'profile_score', 'package(lpa)'])\ndf","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T11:30:37.192789Z","iopub.execute_input":"2025-01-02T11:30:37.193215Z","iopub.status.idle":"2025-01-02T11:30:37.217679Z","shell.execute_reply.started":"2025-01-02T11:30:37.193184Z","shell.execute_reply":"2025-01-02T11:30:37.216549Z"}},"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"   cgpa  profile_score  package(lpa)\n0     8              8             4\n1     7              9             5\n2     6             10             6\n3     5             12             7","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>cgpa</th>\n      <th>profile_score</th>\n      <th>package(lpa)</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>8</td>\n      <td>8</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>7</td>\n      <td>9</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>6</td>\n      <td>10</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>5</td>\n      <td>12</td>\n      <td>7</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"# Function to initialize parameters\n\n# This will take in each layer dimensions and return us the weights and biases\n# Initialization will be weights = 0.1, biases = 0 \n# Initialization can be random as well if we want\n\n# Input is of the form (2, 2, 1) - 2 Input Nodes, 2 Hidden Layer nodes, 1 Output Layer node \n\n# Output is of the form us \n# Wi which contains the weights going into nodes of Layer i\n# bi which contains bias going into the nodes of Layer i\n\ndef initialize_parameters(layer_dims):\n    \n    parameters = {}\n    num_layers = len(layer_dims)\n    \n    for layer_num in range(1, num_layers):\n        parameters[\"W\" + str(layer_num)] =  np.ones((layer_dims[layer_num-1], layer_dims[layer_num])) * 0.1\n        parameters['b' + str(layer_num)] = np.zeros((layer_dims[layer_num], 1))\n\n    return parameters","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T11:30:37.218823Z","iopub.execute_input":"2025-01-02T11:30:37.219080Z","iopub.status.idle":"2025-01-02T11:30:37.229273Z","shell.execute_reply.started":"2025-01-02T11:30:37.219057Z","shell.execute_reply":"2025-01-02T11:30:37.228038Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"initialize_parameters([2,2,1])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T11:30:37.231974Z","iopub.execute_input":"2025-01-02T11:30:37.232311Z","iopub.status.idle":"2025-01-02T11:30:37.254244Z","shell.execute_reply.started":"2025-01-02T11:30:37.232279Z","shell.execute_reply":"2025-01-02T11:30:37.252582Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"{'W1': array([[0.1, 0.1],\n        [0.1, 0.1]]),\n 'b1': array([[0.],\n        [0.]]),\n 'W2': array([[0.1],\n        [0.1]]),\n 'b2': array([[0.]])}"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"# Forward Propagation at each layer\n\n# Activation function will be linear\n\n# Input takes \n# A_prev - Activation (output) from nodes of previous layer \n# W - Weights going into current layer\n# b - Bias going into current layer\n\n# Output will give us the Activations (outputs) going out of the current layer\n\ndef linear_forward(A_prev, W, b):\n    \n    Z = (W.T @ A_prev) + b\n    \n    return Z","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T11:30:37.256685Z","iopub.execute_input":"2025-01-02T11:30:37.257133Z","iopub.status.idle":"2025-01-02T11:30:37.274909Z","shell.execute_reply.started":"2025-01-02T11:30:37.257092Z","shell.execute_reply":"2025-01-02T11:30:37.273121Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# Forward propagation for N layers\n\n# Takes input\n# X - Single input sample from dataset\n# parameters - Parameters of the ANN\n\ndef N_layer_forward(X, paramaters):\n\n    A = X                             # Activation (output) from input layer\n    N = (len(parameters) // 2) + 1    # Number of layers-1. // 2 cuz both weights and biases there in parameters. +1 to include input layer as well               \n\n    \n    for layer_num in range(1, N): \n        A_prev = A                                         # A_prev - output of prev layer\n        W_curr_layer = parameters[\"W\" + str(layer_num)]    # Current layer weights\n        b_curr_layer = parameters[\"b\" + str(layer_num)]    # Current layer biases\n\n        # Printing out stuff\n        # print(\"A\"+str(layer_num-1) + \":\\n\", A_prev)\n        # print(\"W\"+str(layer_num) + \":\\n\", W_curr_layer)\n        # print(\"b\"+str(layer_num) + \":\\n\", b_curr_layer)\n        # print(\"*\" * 20)\n\n        A = linear_forward(A_prev, W_curr_layer, b_curr_layer)    # forward propagation through current layer\n        \n        # print(\"A\" + str(layer_num) + \":\\n\", A)\n        # print(\"*\" * 20)\n        # print()\n        # print()\n        \n    # We return both final layer and second final layer output cuz we need it in gradient calculations (see notes)\n    return A, A_prev","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T11:30:37.276235Z","iopub.execute_input":"2025-01-02T11:30:37.276694Z","iopub.status.idle":"2025-01-02T11:30:37.289522Z","shell.execute_reply.started":"2025-01-02T11:30:37.276609Z","shell.execute_reply":"2025-01-02T11:30:37.288223Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"## Now let us see the training ","metadata":{}},{"cell_type":"markdown","source":"### Initialize parameters","metadata":{}},{"cell_type":"code","source":"parameters = initialize_parameters([2, 2, 1])\nparameters","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T11:30:37.290892Z","iopub.execute_input":"2025-01-02T11:30:37.291335Z","iopub.status.idle":"2025-01-02T11:30:37.316372Z","shell.execute_reply.started":"2025-01-02T11:30:37.291300Z","shell.execute_reply":"2025-01-02T11:30:37.314945Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"{'W1': array([[0.1, 0.1],\n        [0.1, 0.1]]),\n 'b1': array([[0.],\n        [0.]]),\n 'W2': array([[0.1],\n        [0.1]]),\n 'b2': array([[0.]])}"},"metadata":{}}],"execution_count":7},{"cell_type":"markdown","source":"### Select an input sample","metadata":{}},{"cell_type":"code","source":"# Selecting the first sample\n\nX = df[['cgpa', 'profile_score']].values[0].reshape(2,1)    # Shape(no of features,1)\ny = df[['package(lpa)']].values[0][0]\n\nX, y","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T11:30:37.317820Z","iopub.execute_input":"2025-01-02T11:30:37.318317Z","iopub.status.idle":"2025-01-02T11:30:37.348572Z","shell.execute_reply.started":"2025-01-02T11:30:37.318279Z","shell.execute_reply":"2025-01-02T11:30:37.346917Z"}},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"(array([[8],\n        [8]]),\n 4)"},"metadata":{}}],"execution_count":8},{"cell_type":"markdown","source":"### Forward propagation","metadata":{}},{"cell_type":"code","source":"y_pred, A1 = N_layer_forward(X, parameters)\ny_pred ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T11:30:37.349928Z","iopub.execute_input":"2025-01-02T11:30:37.350329Z","iopub.status.idle":"2025-01-02T11:30:37.372508Z","shell.execute_reply.started":"2025-01-02T11:30:37.350286Z","shell.execute_reply":"2025-01-02T11:30:37.371201Z"}},"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"array([[0.32]])"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"# A2 - Final layer output (y_pred) \n# A0 - Inputs\n# A1 - Hidden layer inputs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T11:30:37.374305Z","iopub.execute_input":"2025-01-02T11:30:37.374791Z","iopub.status.idle":"2025-01-02T11:30:37.392488Z","shell.execute_reply.started":"2025-01-02T11:30:37.374747Z","shell.execute_reply":"2025-01-02T11:30:37.391013Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"### Calculate Loss","metadata":{}},{"cell_type":"code","source":"loss = (y - y_pred)**2    # MSE\nloss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T11:30:37.393764Z","iopub.execute_input":"2025-01-02T11:30:37.394172Z","iopub.status.idle":"2025-01-02T11:30:37.413851Z","shell.execute_reply.started":"2025-01-02T11:30:37.394141Z","shell.execute_reply":"2025-01-02T11:30:37.412432Z"}},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"array([[13.5424]])"},"metadata":{}}],"execution_count":11},{"cell_type":"markdown","source":"### Update parameters","metadata":{}},{"cell_type":"code","source":"parameters['W2'][0].item()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T11:30:37.415372Z","iopub.execute_input":"2025-01-02T11:30:37.415864Z","iopub.status.idle":"2025-01-02T11:30:37.435888Z","shell.execute_reply.started":"2025-01-02T11:30:37.415819Z","shell.execute_reply":"2025-01-02T11:30:37.434386Z"}},"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"0.1"},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"# Function to update parameters \n\n# Note - This is for regression. Gradients are calculated using Linear Activation and MSE Loss\n# Activation functions of all the nodes are taken as linear here\n\n# From the derivations seen in notes\n# A1 - output from the hidden layer - told ya we will need it \n\ndef update_parameters(parameters, y, y_pred, A1, X, lr):\n    \n    parameters['W2'][0][0] = parameters['W2'][0][0] + (lr * 2 * (y - y_pred)*A1[0][0])\n    parameters['W2'][1][0] = parameters['W2'][1][0] + (lr * 2 * (y - y_pred)*A1[1][0])\n    parameters['b2'][0][0] = parameters['W2'][1][0] + (lr * 2 * (y - y_pred))\n    \n    parameters['W1'][0][0] = parameters['W1'][0][0] + (lr * 2 * (y - y_pred)*parameters['W2'][0][0]*X[0][0])\n    parameters['W1'][0][1] = parameters['W1'][0][1] + (lr * 2 * (y - y_pred)*parameters['W2'][0][0]*X[1][0])\n    parameters['b1'][0][0] = parameters['b1'][0][0] + (lr * 2 * (y - y_pred)*parameters['W2'][0][0])\n    \n    parameters['W1'][1][0] = parameters['W1'][1][0] + (lr * 2 * (y - y_pred)*parameters['W2'][1][0]*X[0][0])\n    parameters['W1'][1][1] = parameters['W1'][1][1] + (lr * 2 * (y - y_pred)*parameters['W2'][1][0]*X[1][0])\n    parameters['b1'][1][0] = parameters['b1'][1][0] + (lr * 2 * (y - y_pred)*parameters['W2'][1][0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T11:30:37.440249Z","iopub.execute_input":"2025-01-02T11:30:37.440748Z","iopub.status.idle":"2025-01-02T11:30:37.457438Z","shell.execute_reply.started":"2025-01-02T11:30:37.440706Z","shell.execute_reply":"2025-01-02T11:30:37.456036Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"y_pred","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T11:30:37.458987Z","iopub.execute_input":"2025-01-02T11:30:37.459385Z","iopub.status.idle":"2025-01-02T11:30:37.488766Z","shell.execute_reply.started":"2025-01-02T11:30:37.459355Z","shell.execute_reply":"2025-01-02T11:30:37.487448Z"}},"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"array([[0.32]])"},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"y_pred = y_pred[0][0]    # extracting element from array\nupdate_parameters(parameters, y, y_pred, A1, X, 0.01)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T11:30:37.490083Z","iopub.execute_input":"2025-01-02T11:30:37.490501Z","iopub.status.idle":"2025-01-02T11:30:37.508403Z","shell.execute_reply.started":"2025-01-02T11:30:37.490456Z","shell.execute_reply":"2025-01-02T11:30:37.506881Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"parameters","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T11:30:37.509250Z","iopub.execute_input":"2025-01-02T11:30:37.509643Z","iopub.status.idle":"2025-01-02T11:30:37.531817Z","shell.execute_reply.started":"2025-01-02T11:30:37.509604Z","shell.execute_reply":"2025-01-02T11:30:37.530213Z"}},"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"{'W1': array([[0.22821709, 0.22821709],\n        [0.22821709, 0.22821709]]),\n 'b1': array([[0.01602714],\n        [0.01602714]]),\n 'W2': array([[0.21776],\n        [0.21776]]),\n 'b2': array([[0.29136]])}"},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"# We can see that the parameters have been updated\n\n# We just have to do the entire thing for all the samples once\n# Then we get one epoch ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T11:30:37.532720Z","iopub.execute_input":"2025-01-02T11:30:37.533110Z","iopub.status.idle":"2025-01-02T11:30:37.553780Z","shell.execute_reply.started":"2025-01-02T11:30:37.533076Z","shell.execute_reply":"2025-01-02T11:30:37.552396Z"}},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":"## Implementing Epochs","metadata":{}},{"cell_type":"code","source":"# epochs implementation\n\nepochs = 5\nparameters = initialize_parameters([2,2,1])\n\nfor epoch in range(epochs):\n\n    loss = []     # to calculate average loss of all samples each epoch\n    \n    # Perform update for each sample\n    for i in range(df.shape[0]): \n\n        # Select one sample \n        X = df[['cgpa', 'profile_score']].values[i].reshape(2, 1) # Shape(no of features, no. of training example)\n        y = df[['package(lpa)']].values[i][0]\n\n        # Forward Propagation\n        y_pred,A1 = N_layer_forward(X,parameters)\n        y_pred = y_pred[0][0]\n\n        # Update Parameters\n        update_parameters(parameters, y, y_pred, A1, X, 0.001)\n        loss.append((y - y_pred) ** 2)\n    \n    print('Epoch-', epoch, 'Loss - ', np.array(loss).mean())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T11:30:37.554997Z","iopub.execute_input":"2025-01-02T11:30:37.555419Z","iopub.status.idle":"2025-01-02T11:30:37.604822Z","shell.execute_reply.started":"2025-01-02T11:30:37.555355Z","shell.execute_reply":"2025-01-02T11:30:37.603715Z"}},"outputs":[{"name":"stdout","text":"Epoch- 0 Loss -  25.321744156025517\nEpoch- 1 Loss -  18.320004165722047\nEpoch- 2 Loss -  9.473661050729628\nEpoch- 3 Loss -  3.2520938634031613\nEpoch- 4 Loss -  1.3407132589299962\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"parameters","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T11:30:37.605873Z","iopub.execute_input":"2025-01-02T11:30:37.606260Z","iopub.status.idle":"2025-01-02T11:30:37.628544Z","shell.execute_reply.started":"2025-01-02T11:30:37.606224Z","shell.execute_reply":"2025-01-02T11:30:37.627503Z"}},"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"{'W1': array([[0.26507636, 0.38558861],\n        [0.27800387, 0.40980287]]),\n 'b1': array([[0.02749056],\n        [0.02974394]]),\n 'W2': array([[0.41165744],\n        [0.48302736]]),\n 'b2': array([[0.48646246]])}"},"metadata":{}}],"execution_count":19},{"cell_type":"markdown","source":"### Welp thats it thats entire backpropagation for you","metadata":{}},{"cell_type":"markdown","source":"## For Classification\n\nExact same process.\nOnly difference is\n- Activation function is Sigmoid\n- Loss is Log Loss (Binary Cross Entropy)","metadata":{}},{"cell_type":"code","source":"# Activation function\ndef sigmoid(Z):\n  A = 1 / (1 + np.exp(-Z))\n  return A\n\n# Forward propagation \ndef sigmoid_forward(A_prev, W, b):\n  \n  Z = np.dot(W.T, A_prev) + b\n  A = sigmoid(Z)                      # added activation function \n  \n  return A","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T12:24:03.082395Z","iopub.execute_input":"2025-01-02T12:24:03.082760Z","iopub.status.idle":"2025-01-02T12:24:03.089390Z","shell.execute_reply.started":"2025-01-02T12:24:03.082733Z","shell.execute_reply":"2025-01-02T12:24:03.087683Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"# N Layer forward propagation\n# Remains same\n\ndef N_layer_forward(X, paramaters):\n\n    A = X                             \n    N = (len(parameters) // 2) + 1                                \n\n    for layer_num in range(1, N): \n        A_prev = A                                         \n        W_curr_layer = parameters[\"W\" + str(layer_num)]    \n        b_curr_layer = parameters[\"b\" + str(layer_num)]    \n\n        A = linear_forward(A_prev, W_curr_layer, b_curr_layer)    \n\n    return A, A_prev","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Function to update weights\n\n# Based on the calculations done in notes\n# Activation - Sigmoid, Loss - Binary Cross Entropy\n\ndef update_parameters(parameters, y, y_pred, A1, X, lr):\n  parameters['W2'][0][0] = parameters['W2'][0][0] + (lr * (y - y_pred)*A1[0][0])\n  parameters['W2'][1][0] = parameters['W2'][1][0] + (lr * (y - y_pred)*A1[1][0])\n  parameters['b2'][0][0] = parameters['W2'][1][0] + (lr * (y - y_pred))\n\n  parameters['W1'][0][0] = parameters['W1'][0][0] + (lr * (y - y_pred)*parameters['W2'][0][0]*A1[0][0]*(1-A1[0][0])*X[0][0])\n  parameters['W1'][0][1] = parameters['W1'][0][1] + (lr * (y - y_pred)*parameters['W2'][0][0]*A1[0][0]*(1-A1[0][0])*X[1][0])\n  parameters['b1'][0][0] = parameters['b1'][0][0] + (lr * (y - y_pred)*parameters['W2'][0][0]*A1[0][0]*(1-A1[0][0]))\n\n  parameters['W1'][1][0] = parameters['W1'][1][0] + (lr * (y - y_pred)*parameters['W2'][1][0]*A1[1][0]*(1-A1[1][0])*X[0][0])\n  parameters['W1'][1][1] = parameters['W1'][1][1] + (lr * (y - y_pred)*parameters['W2'][1][0]*A1[1][0]*(1-A1[1][0])*X[1][0])\n  parameters['b1'][1][0] = parameters['b1'][1][0] + (lr * (y - y_pred)*parameters['W2'][1][0]*A1[1][0]*(1-A1[1][0]))","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}